{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af413b8",
   "metadata": {},
   "source": [
    "# Local Llama2 Document Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a3fc9",
   "metadata": {},
   "source": [
    "This example demonstrates a RAG workflow that uses locally-stored documents to augment the result of an LLM query. Notably, this entire workflow is implemented with locally-hosted models and open source utilities. This will be especially useful for RAG workflows that involve sensitive data (medical, legal, financial) which you might not want to send to OpenAI or Cohere for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb8e317",
   "metadata": {},
   "source": [
    "## Set up the RAG workflow environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4e1c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chromadb package is not available on this system, skipping\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from llama_index.core import ServiceContext, set_global_service_context, set_global_handler, SimpleDirectoryReader\n",
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utils.hosting_utils import RAGLLM\n",
    "from utils.rag_utils import RAGEmbedding\n",
    "from utils.storage_utils import RAGIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43490a5",
   "metadata": {},
   "source": [
    "Set up some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa721ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab705cd1",
   "metadata": {},
   "source": [
    "## Bring up a locally-hosted Llama2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be61323",
   "metadata": {},
   "source": [
    "Define the RAG configuration. You should try modifying the configuration values below to see how it affects the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117d1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_cfg = {\n",
    "    # Node parser config\n",
    "    \"chunk_size\": 256,\n",
    "    \"chunk_overlap\": 0,\n",
    "\n",
    "    # Embedding model config\n",
    "    \"embed_model_type\": \"hf\",\n",
    "    \"embed_model_name\": \"BAAI/bge-base-en-v1.5\",\n",
    "\n",
    "    # LLM config\n",
    "    \"llm_type\": \"local\",\n",
    "    \"llm_name\": \"Llama-2-7b-chat-hf\",\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"top_k\": 50,\n",
    "    \"do_sample\": False,\n",
    "\n",
    "    # Vector DB config\n",
    "    \"vector_db_type\": \"chromadb\",\n",
    "    \"vector_db_name\": \"local_llama2\",\n",
    "\n",
    "    # Retriever and query config\n",
    "    \"retriever_type\": \"vector_index\", # \"vector_index\", \"bm25\"\n",
    "    \"retriever_similarity_top_k\": 3,\n",
    "    \"query_mode\": \"hybrid\", # \"default\", \"hybrid\"\n",
    "    \"hybrid_search_alpha\": 0.5, # float from 0.0 (sparse search - bm25) to 1.0 (vector search)\n",
    "    \"response_mode\": \"compact\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3875ff",
   "metadata": {},
   "source": [
    "Load a Llama2 LLM for generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "907255b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local LLM model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8ffd3d7af342e187007c4bc0dd6ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Can we check if the model is already loaded?\n",
    "llm = RAGLLM(rag_cfg['llm_type'], rag_cfg['llm_name']).load_model(**rag_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726b580",
   "metadata": {},
   "source": [
    "## Start with a basic generation request without RAG augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd204f8b",
   "metadata": {},
   "source": [
    "Let's start by asking Llama2 a difficult, domain-specific question we don't expect it to have an answer to. A simple question like \"*What is the capital of France?*\" is not a good question here, because that's basic knowledge that we expect the LLM to know.\n",
    "\n",
    "Instead, we want to ask it a question that is very domain-specific that it won't know the answer to. A good example would an obscure detail buried deep within a company's annual report. For example:\n",
    "\n",
    "\"*How many Vector scholarships in AI were awarded in 2022?*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ecc11e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many Vector scholarships in AI were awarded in 2022?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01a2f2",
   "metadata": {},
   "source": [
    "Now, send the generation request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4805e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "According to the Vector Institute's website, in 2022, the Vector Institute awarded 15 scholarships in AI.\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aea26e",
   "metadata": {},
   "source": [
    "Without additional information, our local Llama2 model is unable to answer the question correctly. **Vector in fact awarded 109 AI scholarships in 2022**. Fortunately, we do have that information available in Vector's 2021-22 Annual Report, which is available in the source-materials folder. Let's see how we can use RAG to augment our question with a document search and get the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36ed1b",
   "metadata": {},
   "source": [
    "## Ingestion: Load and store the documents from source-materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47e0b80",
   "metadata": {},
   "source": [
    "Start by reading in all the PDF files from source-materials, break them up into smaller digestible chunks, then encode them as vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aadf90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source materials: 42\n",
      "\n",
      "Example first source material:\n",
      " Doc ID: 3c6341f4-9e9b-4b7d-aeda-b82b76b5ff5d\n",
      "Text: 2021â€“2022  ANNUAL  REPORT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the pdfs\n",
    "pdf_folder_path = \"./source_documents\"\n",
    "documents = SimpleDirectoryReader(pdf_folder_path).load_data()\n",
    "print(f\"Number of source materials: {len(documents)}\\n\")\n",
    "print(f\"Example first source material:\\n {documents[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35954672",
   "metadata": {},
   "source": [
    "Load node parser to split documents into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eae02ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceSplitter(chunk_size=rag_cfg['chunk_size'], chunk_overlap=rag_cfg['chunk_overlap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc440fcf",
   "metadata": {},
   "source": [
    "Load embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3beb330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hf embedding model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b057db493724e379d31de617147f4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084c4f6c31664fe8adbf7c884f5e35ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e6b3ac3ce9416a947146e674fb0e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d7a32ab50d4283a223c1d966e6bdfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0ac8819e6f4388a23b2745d9647e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d812aa9271d465ba1cb70cb98e3aecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_model = RAGEmbedding(\n",
    "    model_type=rag_cfg['embed_model_type'], \n",
    "    model_name=rag_cfg['embed_model_name']\n",
    ").load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a46951",
   "metadata": {},
   "source": [
    "Use service context to set the node parser, embedding model, LLM, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb555661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4031/798618222.py:1: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    node_parser=node_parser,\n",
    "    embed_model=embed_model,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Set it globally to avoid passing it to every class, this sets it even for rag_utils.py\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2dc715",
   "metadata": {},
   "source": [
    "Create index using the appropriate vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "599d5946",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chromadb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mRAGIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrag_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvector_db_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrag_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvector_db_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/projects/aieng/public/rag_bootcamp/local_llama2/../utils/storage_utils.py:37\u001b[0m, in \u001b[0;36mRAGIndex.create_index\u001b[0;34m(self, docs, save, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_index\u001b[39m(\u001b[38;5;28mself\u001b[39m, docs, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Only supports ChromaDB and Weaviate as of now\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchromadb\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 37\u001b[0m         chroma_client \u001b[38;5;241m=\u001b[39m \u001b[43mchromadb\u001b[49m\u001b[38;5;241m.\u001b[39mClient()\n\u001b[1;32m     38\u001b[0m         chroma_collection \u001b[38;5;241m=\u001b[39m chroma_client\u001b[38;5;241m.\u001b[39mcreate_collection(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_name)\n\u001b[1;32m     39\u001b[0m         vector_store \u001b[38;5;241m=\u001b[39m ChromaVectorStore(chroma_collection\u001b[38;5;241m=\u001b[39mchroma_collection)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chromadb' is not defined"
     ]
    }
   ],
   "source": [
    "index = RAGIndex(db_type=rag_cfg['vector_db_type'], db_name=rag_cfg['vector_db_name']).create_index(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ce508",
   "metadata": {},
   "source": [
    "## Now perform the RAG-augmented generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d66a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_local",
   "language": "python",
   "name": "rag_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
